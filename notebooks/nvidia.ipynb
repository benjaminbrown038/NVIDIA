{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBnb9CTLCbXV28gsRy7MKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminbrown038/NVIDIA/blob/main/notebooks/nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NVIDIA\n",
        "\n",
        "- Multi-GPU Training Strategies\n",
        "- Unoptimized deployment of GPT-J\n",
        "- Optimizing inference with NVIDIA FasterTransformer library\n",
        "- Multi-Node Distributed Training Strategies\n",
        "- GPT_LM_pretrainings_optimizations\n",
        "- GPT-J deployment with NVIDIA FasterTransformer and Triton Inference server\n",
        "- Multi-Nodes Distributed Training for Computer Vision\n",
        "- Mixture of Experts (MoE)\n",
        "- Sequence Data"
      ],
      "metadata": {
        "id": "c32iwcGm_Ge0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-GPU Training Strategies"
      ],
      "metadata": {
        "id": "ow1S9pcr_Ghr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "1HHVdzXt_GtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scancel -u $USER"
      ],
      "metadata": {
        "id": "d-lt2xl7B5_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "ybL6rm60B6Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /dli/code/pretrain_gpt_1GPU.sh"
      ],
      "metadata": {
        "id": "rgUy3bAoB6Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<pre>\n",
        "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
        "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
        "   Step 3: Run the megatron gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_1GPU.sh</font>\n",
        "</pre>"
      ],
      "metadata": {
        "id": "bIDl5hUMB6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "Wi-W1ZRwB6LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 6m\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RfNYdRcqB6N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep iteration /dli/megatron/logs/log_1GPU.txt"
      ],
      "metadata": {
        "id": "L2nLXq4hB6Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /dli/megatron/checkpoints/*"
      ],
      "metadata": {
        "id": "OMVh502HB6Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /dli/code/pretrain_gpt_2GPU.sh"
      ],
      "metadata": {
        "id": "8v_PjaY1CkFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash"
      ],
      "metadata": {
        "id": "49QZ11gdCmTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Distributed training args\n",
        "NNODES=1\n",
        "GPUS_PER_NODE=#FIXEME         # <--- CHANGE HERE\n",
        "TP_SIZE=1\n",
        "PP_SIZE=1\n",
        "\n",
        "# Distributed training\n",
        "MICRO_BATCH_SIZE=2\n",
        "GLOBAL_BATCH_SIZE=#FIXEME    # <--- CHANGE HERE\n",
        "\n",
        "# Model architecture\n",
        "NLAYERS=12\n",
        "NHIDDEN=768\n",
        "NHEADS=32\n",
        "SEQ_LEN=1024\n",
        "VOCAB_SIZE=50257\n",
        "\n",
        "# Data Paths\n",
        "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
        "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
        "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
        "\n",
        "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
        "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
        "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
        "LOGS_PATH=/dli/megatron/logs\n",
        "NAME=\"log_2GPU\"\n",
        "\n",
        "# SLURM args\n",
        "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
        "MASTER_PORT=6000\n",
        "\n",
        "OPTIMIZER_ARGS=\" \\\n",
        "            --optimizer adam \\\n",
        "            --adam-beta1 0.9 \\\n",
        "            --adam-beta2 0.95 \\\n",
        "            --adam-eps 1e-8 \\\n",
        "            --lr 6e-5 \\\n",
        "            --min-lr 6e-6 \\\n",
        "            --lr-decay-style cosine \\\n",
        "            --lr-decay-iters 800 \\\n",
        "            --lr-warmup-fraction .01 \\\n",
        "            --clip-grad 1.0 \\\n",
        "            --weight-decay 1e-1 \\\n",
        "            --exit-duration-in-mins 1190 \\\n",
        "            \"\n",
        "\n",
        "GPT_ARGS=\" \\\n",
        "            --num-layers $NLAYERS \\\n",
        "            --hidden-size $NHIDDEN \\\n",
        "            --num-attention-heads $NHEADS \\\n",
        "            --seq-length $SEQ_LEN \\\n",
        "            --max-position-embeddings $SEQ_LEN \\\n",
        "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
        "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
        "            --train-iters 100 \\\n",
        "            --vocab-file $VOCAB_FILE \\\n",
        "            --merge-file $MERGE_FILE \\\n",
        "            --init-method-std 0.006 \\\n",
        "            $OPTIMIZER_ARGS \\\n",
        "            $EXIT_OPTS \\\n",
        "            \"\n",
        "\n",
        "OUTPUT_ARGS=\" \\\n",
        "            --log-interval 10 \\\n",
        "            --save-interval 300 \\\n",
        "            --eval-interval 1000 \\\n",
        "            --eval-iters 10 \\\n",
        "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
        "            --tensorboard-queue-size 1 \\\n",
        "            --log-timers-to-tensorboard \\\n",
        "            --log-batch-size-to-tensorboard \\\n",
        "            --log-validation-ppl-to-tensorboard \\\n",
        "            \"\n",
        "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
        "            --nproc_per_node $GPUS_PER_NODE \\\n",
        "            --nnodes $NNODES \\\n",
        "            --master_addr $MASTER_ADDR \\\n",
        "            --master_port $MASTER_PORT \\\n",
        "\n",
        "export CMD=\" \\\n",
        "            /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
        "            --tensor-model-parallel-size $TP_SIZE \\\n",
        "            --pipeline-model-parallel-size $PP_SIZE \\\n",
        "            $GPT_ARGS \\\n",
        "            $OUTPUT_ARGS \\\n",
        "            --save $CHECKPOINT_PATH \\\n",
        "            --data-path $DATA_PATH \\\n",
        "            --data-impl mmap \\\n",
        "            --split 949,50,1 \\\n",
        "            --distributed-backend nccl \\\n",
        "            \"\n",
        "\n",
        "bash -c '$LAUNCHER  $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
      ],
      "metadata": {
        "id": "XUX5aINCB6Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<pre>\n",
        "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
        "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
        "   Step 3: Run the megatron gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_2GPU.sh</font>\n",
        "</pre>"
      ],
      "metadata": {
        "id": "piSb82SmB6ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "t57bKykjB6b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "CjDx5PtzCYzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep iteration /dli/megatron/logs/log_2GPU.txt"
      ],
      "metadata": {
        "id": "k5v4ZLO9CY43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /dli/megatron/checkpoints/*"
      ],
      "metadata": {
        "id": "VfoEp9KECY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "iRN51DWqCY-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scancel -u $USER"
      ],
      "metadata": {
        "id": "f3Gv05A5CZBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "4ExOQCzaCZDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unoptimized Deployment of GPT-J"
      ],
      "metadata": {
        "id": "NEM4qYK4_GzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "K4FlvlTI_G88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"./weights/gpt-j/hf/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./weights/gpt-j/hf/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./weights/gpt-j/hf/\")"
      ],
      "metadata": {
        "id": "iuePKAhADCPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\")\n",
        "model.half().to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "CAJcUnfeDCSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate a random sentence.\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids=None, max_length=128, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "LqQHJIvVDCVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the generated text\n",
        "for sentence in output:\n",
        "    sentence = sentence.tolist()\n",
        "    text = tokenizer.decode(sentence, clean_up_tokenization_spaces=True)\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "1xcjDN6jDCZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"English: I do not speak French. French: Je ne parle pas français.\" \\\n",
        "                             \"English: See you later! French: À tout à l'heure!\" \\\n",
        "                             \"English: Where is a good restaurant? French: Où est un bon restaurant?\" \\\n",
        "                             \"English: What rooms do you have available? French:\", return_tensors=\"pt\").cuda(0)"
      ],
      "metadata": {
        "id": "B0eeAd7tDCcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids=input_ids, max_length=82, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "zX_0I19BDCfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = output[0].tolist()\n",
        "text = tokenizer.decode(sentence, clean_up_tokenization_spaces=True)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "651HT59xDCiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids=input_ids, max_length=80, num_return_sequences=1, num_beams=5, temperature=0.7, repetition_penalty=3.0, pad_token_id=tokenizer.eos_token_id)\n",
        "sentence = output[0].tolist()\n",
        "text = tokenizer.decode(sentence, clean_up_tokenization_spaces=True)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "hJD-PzeVDCmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"Create an SQL request to find all users that live in Califorian and have more than 1000 credits.\", r)"
      ],
      "metadata": {
        "id": "vBVk1p0qDCpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids=input_ids, max_length=82, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "cHHDbJ9ODCsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids=input_ids, max_length=80, num_return_sequences=1, num_beams=5, temperature=0.7, repetition_penalty=3.0, pad_token_id=tokenizer.eos_token_id)\n",
        "sentence = output[0].tolist()\n",
        "text = tokenizer.decode(sentence, clean_up_tokenization_spaces=True)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "6_XRzaYbDCvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "execution_time = 0\n",
        "num_iterations = 10\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_iterations):\n",
        "        start = time.time()\n",
        "        output = model.generate(input_ids=None, max_length=128, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, eos_token_id=50256)\n",
        "        end = time.time()\n",
        "        execution_time += end - start"
      ],
      "metadata": {
        "id": "c1foc9O8DZuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average inference time of 128 tokens is:\",\n",
        "     1000 * (execution_time/float(num_iterations)), \"ms\")"
      ],
      "metadata": {
        "id": "eP0FyZKxDZxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing inference with NVIDIA FasterTransformer library"
      ],
      "metadata": {
        "id": "oUU4U00__HG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the SLURM jobs queue\n",
        "!squeue"
      ],
      "metadata": {
        "id": "VX4Z4tXU_HNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cancel admin user jobs\n",
        "!scancel -u $USER\n",
        "\n",
        "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
        "!squeue"
      ],
      "metadata": {
        "id": "JBhP5IwMDp5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the relevant libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import display_html\n",
        "\n",
        "def restartkernel() :\n",
        "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
        "\n",
        "# define an image transform\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5),(0.5, 0.5, 0.5))"
      ],
      "metadata": {
        "id": "O8LA_UuzDp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CIFAR10 training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "Naxb7Z-JDqBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CIFAR10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                       train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=64,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=2)"
      ],
      "metadata": {
        "id": "ROQiCMBDDqD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some random training images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(images, labels):\n",
        "    for i in range(8):\n",
        "        img = images[i] / 2 + 0.5\n",
        "        npimg = img.numpy()\n",
        "        plt.subplot(2,4,i+1)\n",
        "        plt.imshow(np.transpose(npimg, (1, 2 , 0)));\n",
        "        plt.axis('off');\n",
        "        plt.title(classes[labels[i]])\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# get some training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "# Show images\n",
        "imshow(images,labels)"
      ],
      "metadata": {
        "id": "_wrxadl9DqGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "cnn_net = CNN_Net()"
      ],
      "metadata": {
        "id": "-d3O81aBDqJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the model to GPU 0\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn_net.to(device)"
      ],
      "metadata": {
        "id": "AGuZD_f0DqMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's have a look at the Convolutional Neural Network\n",
        "from torchsummary import summary\n",
        "summary(cnn_net,input_size=(3,32,32))"
      ],
      "metadata": {
        "id": "mUI1Jlc9DqPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnn_net.parameters(), lr=0.001,momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Tensorboard event recording directory\n",
        "writer = SummaryWriter('megatron/tensorboard/cifar10')\n",
        "\n",
        "log_interval=100\n",
        "batch_size=64\n",
        "epochs=2"
      ],
      "metadata": {
        "id": "MJan4r4RDqSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cnn_net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print the loss and accuracy metrics log_interval mini-batches\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        if i % log_interval == (log_interval - 1):\n",
        "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
        "            writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
        "            writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "            running_loss = 0.0\n",
        "    # print the last iterations\n",
        "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
        "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
        "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "\n",
        "print('Training Done')\n",
        "writer.add_graph(cnn_net, inputs)\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "GmFpeGvdDqVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%js\n",
        "const href = window.location.hostname +'/tensorboard/';\n",
        "let a = document.createElement('a');\n",
        "let link = document.createTextNode('Open Tensorboard!');\n",
        "a.appendChild(link);\n",
        "a.href = \"http://\" + href;\n",
        "a.style.color = \"navy\"\n",
        "a.target = \"_blank\"\n",
        "element.append(a);"
      ],
      "metadata": {
        "id": "vWQdMoALDqYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = cnn_net(images.to(device))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "        c = (predicted == labels.to(device)).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %2f %%' %\n",
        "      (100 * correct / total))\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2f %%' %\n",
        "          (classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "metadata": {
        "id": "4CKB5sRFD_hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net_Parallel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_Parallel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5).to('cuda:0')               # Changed here\n",
        "        self.pool = nn.MaxPool2d(2, 2).to('cuda:0')                # Changed here\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5).to('cuda:1')              # Changed here\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120).to('cuda:1')         # Changed here\n",
        "        self.fc2 = nn.Linear(120, 84).to('cuda:1')                 # Changed here\n",
        "        self.fc3 = nn.Linear(84, 10).to('cuda:1')                  # Changed here\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x.to('cuda:0'))))          # Changed here\n",
        "        x = self.pool(F.relu(self.conv2(x.to('cuda:1'))))          # Changed here\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "cnn_net_pp = Net_Parallel()"
      ],
      "metadata": {
        "id": "wNJr0C3aD_kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnn_net_pp.parameters(), lr=0.001,momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Tensorboard event recording directory\n",
        "writer_pp = SummaryWriter('megatron/tensorboard/cifar10_PP')\n",
        "\n",
        "log_interval=100\n",
        "batch_size=64\n",
        "epochs=2"
      ],
      "metadata": {
        "id": "e5ecVOd7D_nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "\n",
        "<pre>\n",
        "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
        "   Step 2: Check the GPUs: <font color=\"green\">watch nvidia-smi</font>\n",
        "</pre"
      ],
      "metadata": {
        "id": "prH425AbD_qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN with pipeline parallel\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cnn_net_pp(inputs.to('cuda:0'))                                # Changed here\n",
        "        loss = criterion(outputs, labels.to('cuda:1'))                           # Changed here\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.reset_max_memory_allocated(0)\n",
        "        # print the loss and accuracy metrics log_interval mini-batches\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.to('cuda:1')).sum().item()                 # Changed here\n",
        "        if i % log_interval == (log_interval - 1):\n",
        "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
        "            writer_pp.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
        "            writer_pp.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "            running_loss = 0.0\n",
        "    # print the last iterations\n",
        "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
        "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
        "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "\n",
        "print('Training Done')\n",
        "writer_pp.add_graph(cnn_net_pp, inputs)\n",
        "writer_pp.flush()\n",
        "writer_pp.close()"
      ],
      "metadata": {
        "id": "qgsnidAtD_sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have a look at the DeepSpeed config\n",
        "!cat code/moe/ds_config.json"
      ],
      "metadata": {
        "id": "fBcqSCwDELno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the relevant library\n",
        "import deepspeed\n",
        "\n",
        "# define the argument class, the training arguments, and DeepSpeed\n",
        "class Args:\n",
        "    log_interval=100\n",
        "    batch_size=64\n",
        "    epochs=2\n",
        "    deepspeed = True\n",
        "    deepspeed_config = \"code/moe/ds_config.json\"\n",
        "    local_rank= 0\n",
        "\n",
        "args=Args()"
      ],
      "metadata": {
        "id": "fDyQO34PELqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the CNN network\n",
        "cnn_net_ds = CNN_Net()\n",
        "\n",
        "# Define the hyperparameters\n",
        "parameters = filter(lambda p: p.requires_grad, cnn_net_ds.parameters())\n",
        "\n",
        "# Wrap the CNN network with DeepSpeed\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(args=args, model=cnn_net_ds, model_parameters=parameters, training_data=trainset)\n",
        "\n",
        "# enable mixed precision\n",
        "fp16 = model_engine.fp16_enabled()\n",
        "\n",
        "device = model_engine.local_rank\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Tensorboard event recording directory\n",
        "writer_ds = SummaryWriter('megatron/tensorboard/cifar10_DS')"
      ],
      "metadata": {
        "id": "JwBLkuP-ELtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN with DeepSpeed\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        if fp16:\n",
        "            inputs = inputs.half()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_engine(inputs)             # Changed net_cnn to model_engine\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        model_engine.backward(loss)                # Changed net_cnn to model_engine\n",
        "        model_engine.step()                        # Changed net_cnn to model_engine\n",
        "\n",
        "        # print the loss and accuracy metrics log_interval mini-batches\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        if i % log_interval == (log_interval - 1):\n",
        "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
        "            writer_ds.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
        "            writer_ds.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # print the last iterations\n",
        "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
        "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
        "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
        "\n",
        "print('Training Done')\n",
        "writer_ds.add_graph(model_engine, inputs)\n",
        "writer_ds.flush()\n",
        "writer_ds.close()"
      ],
      "metadata": {
        "id": "yivjgorhELv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill zombie processes\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "G_IyUjjPELyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the training on 4 GPUs with Data parallel\n",
        "!deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed.py \\\n",
        "    --deepspeed \\\n",
        "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
        "    --profile-execution=True \\\n",
        "    --profile-name='zero0'"
      ],
      "metadata": {
        "id": "OQYpZeldDqbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /dli/code/run_cifar10_deepspeed_2Nodes.sh"
      ],
      "metadata": {
        "id": "5fQjAmqxEp3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash"
      ],
      "metadata": {
        "id": "jJ3-fKXoEp6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH --job-name=dli_ds\n"
      ],
      "metadata": {
        "id": "bWKUWGV_EuAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH --nodes=2\n"
      ],
      "metadata": {
        "id": "ybsEfjq4EuDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH --ntasks-per-node=1\n"
      ],
      "metadata": {
        "id": "vUWzlAWwEuGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n"
      ],
      "metadata": {
        "id": "X27wvNs_EuJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH -o /dli/megatron/logs/%j.out\n"
      ],
      "metadata": {
        "id": "Up5WKaJIEuMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SBATCH -e /dli/megatron/logs/%j.err"
      ],
      "metadata": {
        "id": "vpXXzs_gE07A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nodes\n",
        "NUM_NODES=2\n",
        "# Number of GPUs per node\n",
        "NUM_GPUS=2\n",
        "\n",
        "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/code/moe/hostfile --num_gpus=${NUM_GPUS} /dli/code/moe/cifar10_deepspeed.py \\\n",
        "    --deepspeed \\\n",
        "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
        "    --profile-execution=True \\\n",
        "    --profile-name='zero0_sbatch'"
      ],
      "metadata": {
        "id": "VF_eH_-xEVAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the 2 nodes jobs\n",
        "!sbatch /dli/code/run_cifar10_deepspeed_2Nodes.sh\n",
        "\n",
        "# Check the SLURM queue\n",
        "!squeue"
      ],
      "metadata": {
        "id": "14fCn2f-EVDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU utilization on the master node\n",
        "!sleep 10\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TN9lgqK1EVGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DeepSpeed config file for Zero stage 3 Offload\n",
        "!cat /dli/code/moe/ds_config_stage_3.json"
      ],
      "metadata": {
        "id": "rEdM9JejEVJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "net = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet152\", force_reload=True, pretrained=True)"
      ],
      "metadata": {
        "id": "VFtWXv2IEdOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=4 /dli/code/moe/large_model_deepspeed.py \\\n",
        "    --deepspeed \\\n",
        "    --deepspeed_config /dli/code/moe/ds_config_stage_3.json \\\n",
        "    --profile-execution=True \\\n",
        "    --profile-name='zero_resnet152_stage3'"
      ],
      "metadata": {
        "id": "XjJgoQLaEdRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%js\n",
        "const href = window.location.hostname +'/tensorboard/';\n",
        "let a = document.createElement('a');\n",
        "let link = document.createTextNode('Open Tensorboard!');\n",
        "a.appendChild(link);\n",
        "a.href = \"http://\" + href;\n",
        "a.style.color = \"navy\"\n",
        "a.target = \"_blank\"\n",
        "element.append(a);"
      ],
      "metadata": {
        "id": "sp2gUa3KEdVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DeepSpeed config file for Zero stage 1\n",
        "!cat /dli/code/moe/ds_config_stage_1.json"
      ],
      "metadata": {
        "id": "0EwAVAf7EdZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=4 /dli/code/moe/large_model_deepspeed.py \\\n",
        "    --deepspeed \\\n",
        "    --deepspeed_config #FIXEME \\\n",
        "    --profile-execution=True \\\n",
        "    --profile-name=#FIXEME"
      ],
      "metadata": {
        "id": "gS04e_wjEdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!squeue"
      ],
      "metadata": {
        "id": "S9gejVcqEdkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cancel admin user jobs\n",
        "!scancel -u $USER\n",
        "\n",
        "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
        "!squeue"
      ],
      "metadata": {
        "id": "jg4Qpw3TEdnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Node Distributed Training Strategies"
      ],
      "metadata": {
        "id": "9JC_LMeLAK8E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kluL7Iwd_HVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT LM Pretrainings Optimizations"
      ],
      "metadata": {
        "id": "ZtrPzG_3AL2E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGQI1C5y_HdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-J deployment with NVIDIA FasterTransformer and Triton Inference server"
      ],
      "metadata": {
        "id": "5DLouxOSAMf8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TntaMHRYAMl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Nodes Distributed Training for Computer Vision"
      ],
      "metadata": {
        "id": "A87MiDllAMqe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJcci-QvAMu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture of Experts (MoE)"
      ],
      "metadata": {
        "id": "U5_C9PNKAMze"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsxOJ-iWAM4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Data"
      ],
      "metadata": {
        "id": "ldxm_IYkAM8F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iinS-8h_ANVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}