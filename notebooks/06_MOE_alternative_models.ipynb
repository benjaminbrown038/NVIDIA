{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Mixture of Experts (MoE)\n",
    "\n",
    "In this notebook, we will learn about Mixture of Experts model training.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are :\n",
    "* Learn how to incorporate linear experts on a simple Convolutional Network\n",
    "* Learn how to train the new Mixture of Experts CNN for classification\n",
    "\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.1 Mixture of Experts Introduction\n",
    "\n",
    "A Mixture of Experts (MoE) is a neural network where some layers are partitioned into small groups that can be activated or not according to the context. \n",
    "This structure allows the network to learn a wider range of behaviors. The other advantage is that MoE models will require less computation as only few experts are active at a time.\n",
    "\n",
    "<img src=\"images/MOE.png\" width=\"450\" />\n",
    "\n",
    "In the recent literature, several models have been developed following the MoE structure, such as the [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Write the Mixture of Experts for the basline CNN\n",
    "\n",
    "Back to our CNN cifar-10  classifier model. Let's modify it to add 1 MoE layer. The convolutional layers of the CNN extract features, while the later fully connected layers are specialized for the CIFAR-10 classification problem. \n",
    "To add expert layers in the network definition, use the `deepspeed.moe.layer.MoE` as follows (modify the forward pass accordingly):\n",
    "\n",
    "```\n",
    "deepspeed.moe.layer.MoE( hidden_size=<Hidden dimension of the model>, \n",
    "                         expert=<Torch module that defines the expert>, \n",
    "                         num_experts=<Desired number of expert>, \n",
    "                         ep_size=<Desired expert-parallel world size>,\n",
    "                         ...\n",
    "                         )\n",
    "                         \n",
    "```\n",
    "\n",
    "Learn more about the DeepSpeed Mixture of Experts in the [dedicated DeepSpeed documentation.](https://deepspeed.readthedocs.io/en/latest/moe.html) \n",
    "\n",
    "Let's transform the latest fully connected layer `fc3` to a MoE layer in order to evaluate the features extracted from early layers. We will add a final classifier `fc4`.\n",
    "We already prepared the [cifar10_deepspeed_MOE.py](./code/moe/cifar10_deepspeed_MOE.py) script. Letâ€™s run it using 8 experts partitioned on 4 GPUs, which means that each GPU will handle 2 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-27 15:02:53,717] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-06-27 15:02:53,744] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 /dli/code/moe/cifar10_deepspeed_MOE.py --deepspeed --deepspeed_config /dli/code/moe/ds_config.json --moe --ep-world-size 4 --num-experts-per-layer 8 --top-k 1 --noisy-gate-policy RSample --moe-param-group --profile-execution=True --profile-name=zero0_MOE\n",
      "[2023-06-27 15:02:54,772] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2023-06-27 15:02:54,773] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2023-06-27 15:02:56,022] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Traceback (most recent call last):\n",
      "  File \"/dli/code/moe/cifar10_deepspeed_MOE.py\", line 101, in <module>\n",
      "    deepspeed.init_distributed() \n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/utils/distributed.py\", line 51, in init_distributed\n",
      "    torch.distributed.init_process_group(backend=dist_backend,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 578, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "[2023-06-27 15:02:56,793] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 5886\n",
      "[2023-06-27 15:02:56,793] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 5887\n",
      "[2023-06-27 15:02:56,793] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 5888\n",
      "[2023-06-27 15:02:56,794] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 5889\n",
      "[2023-06-27 15:02:56,794] [ERROR] [launch.py:184:sigkill_handler] ['/opt/conda/bin/python3.8', '-u', '/dli/code/moe/cifar10_deepspeed_MOE.py', '--local_rank=3', '--deepspeed', '--deepspeed_config', '/dli/code/moe/ds_config.json', '--moe', '--ep-world-size', '4', '--num-experts-per-layer', '8', '--top-k', '1', '--noisy-gate-policy', 'RSample', '--moe-param-group', '--profile-execution=True', '--profile-name=zero0_MOE'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed_MOE.py  \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --moe \\\n",
    "    --ep-world-size 4 \\\n",
    "    --num-experts-per-layer 8 \\\n",
    "    --top-k 1 \\\n",
    "    --noisy-gate-policy 'RSample' \\\n",
    "    --moe-param-group \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0_MOE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepspeed_MOE.png\" width=\"950\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "The next lab will focus on deploying large neural networks.\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
